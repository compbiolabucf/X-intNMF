{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /*==========================================================================================*\\\n",
    "# **                        _           _ _   _     _  _         _                            **\n",
    "# **                       | |__  _   _/ | |_| |__ | || |  _ __ | |__                         **\n",
    "# **                       | '_ \\| | | | | __| '_ \\| || |_| '_ \\| '_ \\                        **\n",
    "# **                       | |_) | |_| | | |_| | | |__   _| | | | | | |                       **\n",
    "# **                       |_.__/ \\__,_|_|\\__|_| |_|  |_| |_| |_|_| |_|                       **\n",
    "# \\*==========================================================================================*/\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Author: Tien-Thanh Bui (@bu1th4nh)\n",
    "# Title: homework.ipynb\n",
    "# Date: 2023/11/01 18:10:37\n",
    "# Description: Homework File for Prof. Wei Zhang | CS @ UCF\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ydata_profiling as ydp\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os, sys, logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "logging.root.handlers = [];\n",
    "logging.basicConfig(\n",
    "    format   = '%(asctime)s [%(levelname)s] %(message)s', \n",
    "    datefmt  = '%Y/%m/%d %H:%M:%S',\n",
    "    level    = logging.INFO,\n",
    "    handlers = [\n",
    "        # logging.FileHandler(\n",
    "        #     filename = \"data-processing-log_latest.log\", \n",
    "        #     encoding = \"utf-8-sig\",    \n",
    "        #     mode     = \"w\"\n",
    "        # ),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "import torch\n",
    "logging.info(torch.cuda.is_available())\n",
    "\n",
    "import shutil\n",
    "folder = 'checkpoints/'\n",
    "try:\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "            logging.info(f'[TRAIN] Deleted checkpoint {file_path}')\n",
    "        except Exception as e:\n",
    "            logging.error(f'[TRAIN] Failed to delete checkpoint {file_path}. Reason: {e}')\n",
    "except Exception as e: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data - We save the data in parquet format for faster loading\n",
    "if(not os.path.exists('data/Data.parquet')): pd.read_csv('data/Data.csv').to_parquet('data/Data.parquet')\n",
    "if(not os.path.exists('data/Label.parquet')): pd.read_csv('data/Label.csv').to_parquet('data/Label.parquet')\n",
    "\n",
    "# Load\n",
    "data = pd.read_parquet('data/Data.parquet').dropna()\n",
    "logging.info(\"Data\")\n",
    "display(data.head())\n",
    "# Label\n",
    "label = pd.read_parquet('data/Label.parquet')\n",
    "logging.info(\"Label\")\n",
    "display(label.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process: Transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data - We transpose the data to make it easier to work with. \n",
    "# Initially, each row is a feature, and each column is a sample. \n",
    "# We want to make each row a sample, and each column a feature.\n",
    "data = data.set_index('sample').transpose()\n",
    "data.index.name = 'sample'\n",
    "data.columns.name = ''\n",
    "# display(data.head())\n",
    "# Label - We set the index to be the first column - due to the data description. \n",
    "label = label.set_index(label.columns[0])\n",
    "label.index.name = 'sample'\n",
    "# display(label.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Basic Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()\n",
    "logging.info(f\"Number of samples: {data.shape[0]}\")\n",
    "logging.info(f\"Number of features: {data.shape[1]}\")\n",
    "\n",
    "if(not os.path.exists('./description.xlsx')): data.describe().transpose().to_excel('./description.xlsx')\n",
    "description = pd.read_excel('./description.xlsx', index_col=0)  # To save time, we load the description from the file.\n",
    "description.index.name = 'features'\n",
    "# description.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.info()\n",
    "logging.info(f\"Number of LABELED samples: {label.shape[0]}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select k random features from data - I'm feeling lucky =))))\n",
    "# Why? We have 20k features\n",
    "k = 1\n",
    "random_feature_set = np.random.choice(data.columns, k, replace=False)\n",
    "logging.info(f\"Random feature set: {random_feature_set}\")\n",
    "\n",
    "\n",
    "for feature in random_feature_set:\n",
    "    # display(data[feature].describe())\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    data.hist(feature, bins=100, ax=ax[0])\n",
    "    # data[feature].plot.kde(ax=ax[0], secondary_y=True)\n",
    "    ax[0].set_title(f'Histogram for feature {feature}')\n",
    "    data.boxplot(feature, ax=ax[1])\n",
    "    ax[1].set_title(f'Boxplot for feature {feature}')\n",
    "    # fig.savefig(f'./plots/{feature}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Number of sample has ER value = 1: {label[label.ER == 1].shape[0]}\")\n",
    "logging.info(\"Number of sample has ER value = 0: {label[label.ER == 0].shape[0]}\")\n",
    "logging.info(\"Number of sample has TN value = 1: {label[label.TN == 1].shape[0]}\")\n",
    "logging.info(\"Number of sample has TN value = 0: {label[label.TN == 0].shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select k random features from data - I'm feeling lucky :)\n",
    "# Why? We have 20k features, and if we calculate corr. for all of them, the matrix will be too big (400M x size of a float64)\n",
    "k = 20\n",
    "random_feature_set = np.random.choice(data.columns, k, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sample = data[random_feature_set].corr().abs()\n",
    "# display(corr_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distribution of correlation\n",
    "k = 1\n",
    "random_feature_set = np.random.choice(corr_sample.columns, k, replace=False)\n",
    "logging.info(f\"Random feature set: {random_feature_set}\")\n",
    "\n",
    "\n",
    "for feature in random_feature_set:\n",
    "    # display(data[feature].describe())\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "    corr_sample.hist(feature, bins=100, ax=ax[0])\n",
    "    # data[feature].plot.kde(ax=ax[0], secondary_y=True)\n",
    "    ax[0].set_title(f'Histogram of correlation coefficient between {feature} and others')\n",
    "    ax[0].set_xlim(0, 1)\n",
    "\n",
    "    corr_sample[feature].plot.kde(ax=ax[0], secondary_y=True, bw_method='scott')\n",
    "    corr_sample.boxplot(feature, ax=ax[1])\n",
    "    ax[1].set_title(f'Boxplot of correlation coefficient between {feature} and others')\n",
    "    ax[1].set_ylim(0, 1)\n",
    "    # fig.savefig(f'./plots/{feature}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Nullity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Label Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data and label\n",
    "labeled_dataset = data.merge(label[[\"TN\"]], left_index=True, right_index=True, how=\"outer\")\n",
    "labeled_dataset.rename(columns={\"TN\": \"label\"}, inplace=True)\n",
    "\n",
    "logging.info(f\"[Reminder] Data size: {data.shape[0]}\")\n",
    "logging.info(f\"Number of labeled data: {labeled_dataset[labeled_dataset.label.notnull()].shape[0]}\")\n",
    "logging.info(f\"Number of unlabeled data: {labeled_dataset[labeled_dataset.label.isnull()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_dataset = labeled_dataset[labeled_dataset.label.notnull()]\n",
    "usable_data = usable_dataset.drop(columns=[\"label\"])\n",
    "usable_labl = usable_dataset[\"label\"]\n",
    "\n",
    "logging.info(f\"Number of usable data: {usable_dataset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "original_train_data, original_test_data, original_train_labl, original_test_labl = train_test_split(usable_data.values, usable_labl.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Autoencoder Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, layer_characteristics, input_size, learning_rate=1e-3, optimizer=torch.optim.Adam):\n",
    "        \"\"\"\n",
    "            Initialize the Auto Encoder model\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            - `layer_characteristics`: list\n",
    "                List of dictionaries, each dictionary contains the following keys:\n",
    "                    - `layer_type`: \"linear\"\n",
    "                    - `activation`: \"relu\", \"sigmoid\", \"tanh\"\n",
    "                    - `dropout`: 0.0 - 1.0\n",
    "                    - `output_size`: int\n",
    "            - `input_size`: int\n",
    "                Input size of the model. This is the feature length\n",
    "            - `learning_rate`: float, default=1e-3\n",
    "                Learning rate of the optimizer\n",
    "            - `optimizer`: `torch.optim`, default=torch.optim.Adam\n",
    "                Optimizer of the model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"===============================================================\")\n",
    "        logging.info(\"Initializing AutoEncoder\")\n",
    "\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        encoder_size = len(layer_characteristics)\n",
    "\n",
    "        restoration_layer = {\n",
    "            \"output_size\": input_size,\n",
    "            \"layer_type\": layer_characteristics[0][\"layer_type\"],\n",
    "            \"activation\": layer_characteristics[0][\"activation\"],\n",
    "            \"dropout\": layer_characteristics[0][\"dropout\"],\n",
    "        }\n",
    "        layer_characteristics = layer_characteristics + list(reversed(layer_characteristics))[1:] + [restoration_layer]\n",
    "\n",
    "\n",
    "        for i, layer in enumerate(layer_characteristics):\n",
    "            layer_type  = layer[\"layer_type\"]\n",
    "            input_size  = input_size if i == 0 else layer_characteristics[i-1][\"output_size\"]\n",
    "            activation  = layer[\"activation\"]\n",
    "            output_size = layer[\"output_size\"]\n",
    "            dropout     = layer[\"dropout\"]\n",
    "\n",
    "            logging.info(f\"Layer {i}: {input_size} -> {output_size} | {activation} | {layer_type} | {dropout}\")\n",
    "\n",
    "            if i < encoder_size:\n",
    "                # Layer\n",
    "                if(layer_type == \"linear\"): \n",
    "                    self.encoder.append(nn.Linear(input_size, output_size))\n",
    "                    self.encoder.append(nn.Dropout(dropout))\n",
    "                else: raise ValueError(\"Invalid layer type\")\n",
    "                \n",
    "                # Activation\n",
    "                if(activation == \"relu\"): self.encoder.append(nn.ReLU())\n",
    "                elif(activation == \"sigmoid\"): self.encoder.append(nn.Sigmoid())\n",
    "                elif(activation == \"tanh\"): self.encoder.append(nn.Tanh())\n",
    "                else: raise ValueError(\"Invalid activation function\")\n",
    "            else:\n",
    "                # Layer\n",
    "                if(layer_type == \"linear\"): \n",
    "                    self.decoder.append(nn.Linear(input_size, output_size))\n",
    "                    self.decoder.append(nn.Dropout(dropout))\n",
    "                else: raise ValueError(\"Invalid layer type\")\n",
    "                \n",
    "                # Activation\n",
    "                if(activation == \"relu\"): self.decoder.append(nn.ReLU())\n",
    "                elif(activation == \"sigmoid\"): self.decoder.append(nn.Sigmoid())\n",
    "                elif(activation == \"tanh\"): self.decoder.append(nn.Tanh())\n",
    "                else: raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        self.encoder = nn.Sequential(*self.encoder)\n",
    "        self.decoder = nn.Sequential(*self.decoder)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass of the model. This is used for training only.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `x`: torch.Tensor\n",
    "                Input tensor\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "            Inference pass of the model, which only uses encoder part. This is used for dimensionality reduction.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `x`: torch.Tensor\n",
    "                Input tensor\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "            Training step of the model, used in PyTorch Lighning wrapper\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `batch`: tuple\n",
    "                Tuple of input and target tensors\n",
    "            `batch_idx`: int\n",
    "                Batch index\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor, which is the loss value of the model\n",
    "        \"\"\"\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "            Validation step of the model, used in PyTorch Lighning wrapper\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `batch`: tuple\n",
    "                Tuple of input and target tensors\n",
    "            `batch_idx`: int\n",
    "                Batch index\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor, which is the loss value of the model\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoEncoderReducer(layer_characteristics, train_data, test_data, val_data = None):\n",
    "    if(layer_characteristics is None):\n",
    "        layer_characteristics = [\n",
    "            {\n",
    "                \"output_size\": 500,\n",
    "                \"activation\": \"tanh\",\n",
    "                \"layer_type\": \"linear\",\n",
    "                \"dropout\": 0.2,\n",
    "            },\n",
    "            {\n",
    "                \"output_size\": 200,\n",
    "                \"activation\": \"relu\",\n",
    "                \"layer_type\": \"linear\",\n",
    "                \"dropout\": 0.2,\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = AutoEncoder(layer_characteristics, train_data.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "    # Create data loader. We only use training data for training\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = TensorDataset(torch.tensor(train_data, dtype=torch.float32).to(device), torch.tensor(train_data, dtype=torch.float32).to(device))\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1000, \n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor='train_loss', patience=10, mode='min'),\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                monitor='train_loss', \n",
    "                mode='min', \n",
    "                dirpath = \"checkpoints/\", \n",
    "                filename='autoencoder', \n",
    "                save_top_k=1, \n",
    "                verbose=True,\n",
    "                save_weights_only=False,\n",
    "            ),\n",
    "            # pl.callbacks.RichProgressBar(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    \n",
    "\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    model = AutoEncoder.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path,\n",
    "        layer_characteristics=layer_characteristics,\n",
    "        input_size=train_data.shape[1],\n",
    "        strict=False\n",
    "    )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        # Inference\n",
    "        return (\n",
    "            model.inference(torch.tensor(train_data, dtype=torch.float32).to(device)).detach().cpu().numpy(),\n",
    "            model.inference(torch.tensor(test_data, dtype=torch.float32).to(device)).detach().cpu().numpy(), \n",
    "            model.inference(torch.tensor(val_data, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
    "        ) if val_data is not None else (\n",
    "            model.inference(torch.tensor(train_data, dtype=torch.float32).to(device)).detach().cpu().numpy(),\n",
    "            model.inference(torch.tensor(test_data, dtype=torch.float32).to(device)).detach().cpu().numpy(), \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Canonical DR Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAReducer(n_components, train_data, test_data, val_data = None):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=n_components).fit(train_data)\n",
    "    return (\n",
    "        pca.transform(train_data), \n",
    "        pca.transform(test_data),\n",
    "        pca.transform(val_data)\n",
    "    ) if val_data is not None else (\n",
    "        pca.transform(train_data), \n",
    "        pca.transform(test_data)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Canonical ML Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(train_data, train_labl):\n",
    "    from sklearn.svm import SVC\n",
    "    return SVC(probability=True, verbose=True).fit(train_data, train_labl)\n",
    "\n",
    "def RandomForestClassifier(train_data, train_labl):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    return RandomForestClassifier(verbose=True).fit(train_data, train_labl)\n",
    "\n",
    "def LogisticRegressionClassifier(train_data, train_labl):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    return LogisticRegression(max_iter=1000, verbose=True).fit(train_data, train_labl)\n",
    "\n",
    "def AdaBoostClassifier(train_data, train_labl):\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    return AdaBoostClassifier().fit(train_data, train_labl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "\n",
    "class NeuralClassifierCore(pl.LightningModule):\n",
    "    def __init__(self, layer_characteristics, input_size, learning_rate=1e-3, optimizer=torch.optim.Adam):\n",
    "        \"\"\"\n",
    "            Initialize the Binary Neural Classifier model\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            - `layer_characteristics`: list\n",
    "                List of dictionaries, each dictionary contains the following keys:\n",
    "                    - `layer_type`: \"linear\"\n",
    "                    - `activation`: \"relu\", \"sigmoid\", \"tanh\", \"softmax\"\n",
    "                    - `dropout`: 0.0 - 1.0\n",
    "                    - `output_size`: int\n",
    "                Regardless of the number of layers, the network will always have the final feedforward layer with 1 output and sigmoid activation,\n",
    "            - `input_size`: int\n",
    "                Input size of the model. This is the feature length\n",
    "            - `learning_rate`: float, default=1e-3\n",
    "                Learning rate of the optimizer\n",
    "            - `optimizer`: `torch.optim`, default=torch.optim.Adam\n",
    "                Optimizer of the model\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        logging.info(\"===============================================================\")\n",
    "        logging.info(\"Initializing NeuralClassifierCore\")\n",
    "\n",
    "        super(NeuralClassifierCore, self).__init__()\n",
    "\n",
    "        self.net = nn.ModuleList()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        if(layer_characteristics[-1]['output_size'] != 1):\n",
    "            layer_characteristics.append({\n",
    "                \"output_size\": 1,\n",
    "                \"layer_type\": \"linear\",\n",
    "                \"activation\": \"sigmoid\",\n",
    "                \"dropout\": 0.0,\n",
    "            })\n",
    "\n",
    "\n",
    "        for i, layer in enumerate(layer_characteristics):\n",
    "            input_size  = input_size if i == 0 else layer_characteristics[i-1][\"output_size\"]\n",
    "            output_size = layer[\"output_size\"]\n",
    "            layer_type  = layer[\"layer_type\"]\n",
    "            activation  = layer[\"activation\"]\n",
    "            dropout     = layer[\"dropout\"]\n",
    "\n",
    "            logging.info(f\"Layer {i}: {input_size} -> {output_size} | {activation} | {layer_type} | {dropout}\")\n",
    "\n",
    "\n",
    "            # Layer\n",
    "            if(layer_type == \"linear\"): \n",
    "                self.net.append(nn.Linear(input_size, output_size))\n",
    "                self.net.append(nn.Dropout(dropout))\n",
    "            else: raise ValueError(\"Invalid layer type\")\n",
    "            \n",
    "\n",
    "            # Activation\n",
    "            if(activation == \"relu\"): self.net.append(nn.ReLU())\n",
    "            elif(activation == \"sigmoid\"): self.net.append(nn.Sigmoid())\n",
    "            elif(activation == \"tanh\"): self.net.append(nn.Tanh())\n",
    "            elif(activation == \"softmax\"): self.net.append(nn.Softmax())\n",
    "            else: raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass of the model. This is used for training only.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `x`: torch.Tensor\n",
    "                Input tensor\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.net(x).squeeze()\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "            Training step of the model, used in PyTorch Lighning wrapper\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `batch`: tuple\n",
    "                Tuple of input and target tensors\n",
    "            `batch_idx`: int\n",
    "                Batch index\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor, which is the loss value of the model\n",
    "        \"\"\"\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "            Validation step of the model, used in PyTorch Lighning wrapper\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `batch`: tuple\n",
    "                Tuple of input and target tensors\n",
    "            `batch_idx`: int\n",
    "                Batch index\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor, which is the loss value of the model\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "            Testing step of the model, used in PyTorch Lighning wrapper\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            `batch`: tuple\n",
    "                Tuple of input and target tensors\n",
    "            `batch_idx`: int\n",
    "                Batch index\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            torch.Tensor\n",
    "                Output tensor, which is the loss value of the model\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralClassifier(train_data, train_label, val_data, val_label, test_data, test_label, layer_characteristics=None):\n",
    "\n",
    "    logging.info(f\"What is train_data: {train_data.shape}\")\n",
    "\n",
    "    layer_characteristics = layer_characteristics if layer_characteristics is not None else [\n",
    "        {\n",
    "            \"output_size\": np.min([200, train_data.shape[1]]),\n",
    "            \"activation\": \"relu\",\n",
    "            \"layer_type\": \"linear\",\n",
    "            \"dropout\": 0.2,\n",
    "        },\n",
    "        {\n",
    "            \"output_size\": 100,\n",
    "            \"activation\": \"tanh\",\n",
    "            \"layer_type\": \"linear\",\n",
    "            \"dropout\": 0.2,\n",
    "        },\n",
    "        {\n",
    "            \"output_size\": 50,\n",
    "            \"activation\": \"relu\",\n",
    "            \"layer_type\": \"linear\",\n",
    "            \"dropout\": 0.2,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create model\n",
    "    cls = NeuralClassifierCore(layer_characteristics, train_data.shape[1])\n",
    "\n",
    "    # Create data loader\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(np.array(train_data), dtype=torch.float32).to(device), torch.tensor(np.array(train_label), dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(torch.tensor(np.array(val_data), dtype=torch.float32).to(device), torch.tensor(np.array(val_label), dtype=torch.float32).to(device))\n",
    "    test_dataset = TensorDataset(torch.tensor(np.array(test_data), dtype=torch.float32).to(device), torch.tensor(np.array(test_label), dtype=torch.float32).to(device))\n",
    "    \n",
    "    \n",
    "    train = DataLoader(train_dataset, batch_size=64, shuffle=True, persistent_workers=True, num_workers=3)\n",
    "    val = DataLoader(val_dataset, batch_size=64, persistent_workers=True, num_workers=2)\n",
    "    test = DataLoader(test_dataset, batch_size=64, persistent_workers=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=500, \n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min'),            \n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                monitor='val_loss', \n",
    "                mode='min', \n",
    "                dirpath = \"checkpoints/\", \n",
    "                filename='classifier', \n",
    "                save_top_k=1, \n",
    "                verbose=True,\n",
    "                save_weights_only=False,\n",
    "            ),\n",
    "            # pl.callbacks.RichProgressBar(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(cls, train_dataloaders=train, val_dataloaders=val)\n",
    "    cls = NeuralClassifierCore.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path,\n",
    "        layer_characteristics=layer_characteristics,\n",
    "        input_size=train_data.shape[1],\n",
    "        strict=False,\n",
    "    )\n",
    "    logging.info(\"Validation loss:\")    \n",
    "    trainer.test(cls, val)\n",
    "    logging.info(\"Test loss:\")\n",
    "    trainer.test(cls, test)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Inference\n",
    "    return cls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_methods = [\"Not Reduced\", \"PCA\", \"Auto Encoder\"]\n",
    "result_AUC = pd.DataFrame(index = reduction_methods)\n",
    "result_AUC.index.name = \"Reduction Method\"\n",
    "\n",
    "\n",
    "data_dict = {\"Canon ML\": {}, \"Deep Learning\": {}}\n",
    "models_dict = {}\n",
    "\n",
    "\n",
    "AE_characteristics = [\n",
    "    {\n",
    "        \"output_size\": 1000,\n",
    "        \"activation\": \"relu\",\n",
    "        \"layer_type\": \"linear\",\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 500,\n",
    "        \"activation\": \"relu\",\n",
    "        \"layer_type\": \"linear\",\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 200,\n",
    "        \"activation\": \"relu\",\n",
    "        \"layer_type\": \"linear\",\n",
    "        \"dropout\": 0.2,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Canonical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_methods = [\"Random Forest\", \"Logistic Regression\", \"SVM\", \"AdaBoost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, reduction_method) in enumerate(reduction_methods):\n",
    "    # Dimensionality Reduction\n",
    "    if(reduction_method == \"PCA\"): processed_train_data, processed_test_data = PCAReducer(200, original_train_data, original_test_data)\n",
    "    elif(reduction_method == \"Auto Encoder\"): processed_train_data, processed_test_data = AutoEncoderReducer(AE_characteristics, original_train_data, original_test_data)\n",
    "    else: processed_train_data, processed_test_data = original_train_data.copy(), original_test_data.copy()\n",
    "\n",
    "    data_dict[\"Canon ML\"][reduction_method] = dict(\n",
    "        train_data = processed_train_data,\n",
    "        test_data  = processed_test_data,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Classification\n",
    "    for (j, cls_method) in enumerate(cls_methods):\n",
    "        logging.info(f\"Processing {reduction_method} - {cls_method}...\")\n",
    "        logging.info(f\"Feature size: {processed_train_data.shape[1]}, Sample size: {processed_train_data.shape[0]}\")\n",
    "\n",
    "        if(cls_method == \"SVM\"):                    cls = SVMClassifier(processed_train_data, original_train_labl)\n",
    "        elif(cls_method == \"Random Forest\"):         cls = RandomForestClassifier(processed_train_data, original_train_labl)\n",
    "        elif(cls_method == \"Logistic Regression\"):   cls = LogisticRegressionClassifier(processed_train_data, original_train_labl)\n",
    "        elif(cls_method == \"AdaBoost\"):             cls = AdaBoostClassifier(processed_train_data, original_train_labl)\n",
    "        else: raise ValueError(\"Invalid classification method\")\n",
    "\n",
    "        models_dict[reduction_method, cls_method] = cls\n",
    "\n",
    "    logging.info(f\"====================================================\")\n",
    "\n",
    "# Cinderella"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_methods = cls_methods + [\"Deep Learning\"]\n",
    "cls_characteristics = [\n",
    "    {\n",
    "        \"output_size\": 75,\n",
    "        \"activation\": \"relu\",\n",
    "        \"layer_type\": \"linear\",\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 50,\n",
    "        \"activation\": \"tanh\",\n",
    "        \"layer_type\": \"linear\",\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 10,\n",
    "        \"activation\": \"relu\",\n",
    "        \"layer_type\": \"linear\",\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, reduction_method) in enumerate(reduction_methods):\n",
    "    # We further split the training data into training and validation data\n",
    "    dl_train_data, dl_val_data, dl_train_labl, dl_val_labl = train_test_split(original_train_data, original_train_labl, test_size=1/8, random_state=42)\n",
    "\n",
    "    # Dimensionality Reduction\n",
    "    if(reduction_method == \"PCA\"): processed_train_data, processed_test_data, processed_val_data = PCAReducer(200, dl_train_data, original_test_data, dl_val_data)\n",
    "    elif(reduction_method == \"Auto Encoder\"): processed_train_data, processed_test_data, processed_val_data = AutoEncoderReducer(AE_characteristics, dl_train_data, original_test_data, dl_val_data)\n",
    "    else: processed_train_data, processed_test_data, processed_val_data = dl_train_data.copy(), original_test_data.copy(), dl_val_data.copy()\n",
    "\n",
    "    data_dict[\"Deep Learning\"][reduction_method] = dict(\n",
    "        train_data = processed_train_data,\n",
    "        test_data  = processed_test_data,\n",
    "        val_data   = processed_val_data,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Train set size: {processed_train_data.shape}\")\n",
    "    logging.info(f\"Test set size: {processed_test_data.shape}\")\n",
    "    logging.info(f\"Val set size: {processed_val_data.shape}\")\n",
    "\n",
    "    \n",
    "    # Classification\n",
    "    models_dict[reduction_method, \"Deep Learning\"] = NeuralClassifier(processed_train_data, dl_train_labl, processed_val_data, dl_val_labl, processed_test_data, original_test_labl, cls_characteristics)\n",
    "\n",
    "    logging.info(f\"====================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, ax = plt.subplots(\n",
    "    len(reduction_methods), \n",
    "    len(cls_methods), \n",
    "    figsize=(\n",
    "        10 * len(cls_methods),\n",
    "        10 * len(reduction_methods)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for (i, reduction_method) in enumerate(reduction_methods):\n",
    "    for (j, cls_method) in enumerate(cls_methods):\n",
    "\n",
    "\n",
    "        cls = models_dict[reduction_method, cls_method]\n",
    "        if(cls_method == \"Deep Learning\"): \n",
    "            processed_test_data = data_dict[\"Deep Learning\"][reduction_method][\"test_data\"]\n",
    "            predicted = cls(torch.tensor(processed_test_data, dtype=torch.float32)).detach().cpu().numpy()\n",
    "        else: \n",
    "            processed_test_data = data_dict[\"Canon ML\"][reduction_method][\"test_data\"]\n",
    "            predicted = cls.predict_proba(processed_test_data)[::,1]\n",
    "\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(original_test_labl, predicted)\n",
    "        auc_value = auc(fpr, tpr)\n",
    "\n",
    "        result_AUC.loc[reduction_method, cls_method] = auc_value\n",
    "        # Plot ROC curve\n",
    "        if(len(reduction_methods) == 1): axxx = ax[j]\n",
    "        elif(len(cls_methods) == 1): axxx = ax[i]\n",
    "        else: axxx = ax[i, j]\n",
    "        \n",
    "\n",
    "        axxx.plot(list(fpr), list(tpr), label=f\"AUC = {auc_value:.3f}\")\n",
    "        axxx.set_title(f\"{reduction_method} | {cls_method} | {processed_test_data.shape[1]} features | AUC = {auc_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_AUC.to_excel(\"./AUC Result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_AUC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PartOfYourWorld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
