{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /*==========================================================================================*\\\n",
    "# **                        _           _ _   _     _  _         _                            **\n",
    "# **                       | |__  _   _/ | |_| |__ | || |  _ __ | |__                         **\n",
    "# **                       | '_ \\| | | | | __| '_ \\| || |_| '_ \\| '_ \\                        **\n",
    "# **                       | |_) | |_| | | |_| | | |__   _| | | | | | |                       **\n",
    "# **                       |_.__/ \\__,_|_|\\__|_| |_|  |_| |_| |_|_| |_|                       **\n",
    "# \\*==========================================================================================*/\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Author: Bùi Tiến Thành - Tien-Thanh Bui (@bu1th4nh)\n",
    "# Title: main_mofa2.ipynb\n",
    "# Date: 2024/11/07 12:46:46\n",
    "# Description: Baseline implementation for MOFA2 against our model\n",
    "# \n",
    "# (c) 2024 bu1th4nh. All rights reserved. \n",
    "# Written with dedication in the University of Central Florida, EPCOT and the Magic Kingdom.\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import pymongo\n",
    "import logging\n",
    "import numpy as np\n",
    "import mofax as mfx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from s3fs import S3FileSystem\n",
    "from mofapy2.run.entry_point import entry_point\n",
    "from typing import List, Dict, Any, Tuple, Union, Literal\n",
    "from downstream.classification import evaluate_one_target\n",
    "\n",
    "tqdm.pandas()\n",
    "mlflow.set_tracking_uri('http://localhost:6969')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "key = 'bu1th4nh'\n",
    "secret = 'ariel.anna.elsa'\n",
    "endpoint_url = 'http://localhost:9000'\n",
    "\n",
    "s3 = S3FileSystem(\n",
    "    anon=False, \n",
    "    endpoint_url=endpoint_url,\n",
    "    key=key,\n",
    "    secret=secret,\n",
    "    use_ssl=False\n",
    ")\n",
    "storage_option = {\n",
    "    'key': key,\n",
    "    'secret': secret,\n",
    "    'endpoint_url': endpoint_url,\n",
    "}\n",
    "\n",
    "mongo = pymongo.MongoClient(\n",
    "    host='mongodb://localhost',\n",
    "    port=27017,\n",
    "    username='bu1th4nh',\n",
    "    password='ariel.anna.elsa',\n",
    ")\n",
    "mongo_db = mongo['SimilarSampleCrossOmicNMF']\n",
    "\n",
    "\n",
    "configs = [\n",
    "    ('BreastCancer/processed_crossOmics', 'brca', 'BRCA', 'SimilarSampleCrossOmicNMFv3'),\n",
    "    ('LungCancer/processed', 'luad', 'LUAD', 'SimilarSampleCrossOmicNMFv3_LUAD'),\n",
    "    ('OvarianCancer/processed', 'ov', 'OV', 'SimilarSampleCrossOmicNMFv3_OV'),\n",
    "]\n",
    "mofa_latent_dims = 15\n",
    "\n",
    "def find_run(collection, run_id: str, target_id: str): return collection.find_one({'run_id': run_id, 'target_id': target_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_name, res_folder, mongo_collection, mlf_experiment_name in configs:\n",
    "    DATA_PATH = f's3://datasets/{ds_name}'\n",
    "    TARG_PATH = f'{DATA_PATH}/clinical_testdata'\n",
    "    DR_RES_PATH = f's3://results/SimilarSampleCrossOmicNMF/{res_folder}/baseline_MOFA2'\n",
    "    miRNA = pd.read_parquet(f\"{DATA_PATH}/miRNA.parquet\", storage_options=storage_option)\n",
    "    mRNA = pd.read_parquet(f\"{DATA_PATH}/mRNA.parquet\", storage_options=storage_option)\n",
    "\n",
    "    mlflow.set_experiment(mlf_experiment_name)\n",
    "    collection = mongo_db[mongo_collection]\n",
    "\n",
    "    # miRNA.head()\n",
    "    print(\"Dataset: \", ds_name.split('/')[0])\n",
    "    print(\"miRNA\")\n",
    "    print(f\"Sample size: {miRNA.shape[1]}\")\n",
    "    print(f\"Feature size: {miRNA.shape[0]}\")\n",
    "    print(\"mRNA\")\n",
    "    print(f\"Sample size: {mRNA.shape[1]}\")\n",
    "    print(f\"Feature size: {mRNA.shape[0]}\")\n",
    "\n",
    "    display(miRNA.head())\n",
    "    display(mRNA.head())\n",
    "\n",
    "    data_mat = [[miRNA.T.values], [mRNA.T.values]]\n",
    "\n",
    "    Ariel = entry_point()\n",
    "    Ariel.set_data_matrix(\n",
    "        data_mat, \n",
    "        likelihoods=['gaussian', 'gaussian'], \n",
    "        views_names=['miRNA', 'mRNA'],\n",
    "        features_names=[miRNA.index, mRNA.index],\n",
    "        samples_names=[miRNA.columns],\n",
    "    )\n",
    "\n",
    "    Ariel.set_model_options(\n",
    "        factors=mofa_latent_dims\n",
    "    )\n",
    "\n",
    "    Ariel.set_train_options(\n",
    "        convergence_mode = \"fast\",\n",
    "    )\n",
    "\n",
    "    Ariel.build()\n",
    "    Ariel.run()\n",
    "\n",
    "    Ariel.save(\"output.hdf5\")\n",
    "    Belle = mfx.mofa_model(\"output.hdf5\").get_factors(factors=range(mofa_latent_dims), df=True)\n",
    "    \n",
    "    s3.mkdirs(DR_RES_PATH, exist_ok=True)\n",
    "    Belle.to_parquet(f\"{DR_RES_PATH}/H.parquet\", storage_options=storage_option)\n",
    "    \n",
    "    run_id = s3.open(f\"{DR_RES_PATH}/run_id.txt\", 'r').read() if s3.exists(f\"{DR_RES_PATH}/run_id.txt\") else None\n",
    "\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id) if run_id is not None else mlflow.start_run(run_name='baseline_MOFA2'):\n",
    "        if run_id is None: run_id = mlflow.active_run().info.run_id\n",
    "        H = Belle.copy(deep=True)\n",
    "        target_folders = [f's3://{a}' for a in s3.ls(TARG_PATH)]\n",
    "\n",
    "        for target_folder in target_folders:\n",
    "            # Retrieve test data\n",
    "            target_id = str(target_folder.split('/')[-1]).split('.')[0]\n",
    "            if find_run(collection, run_id, target_id) is not None:\n",
    "                logging.info(f\"Run {run_id} on dataset {target_id} already exists. Skipping\")\n",
    "                continue\n",
    "            test_data = pd.read_parquet(target_folder, storage_options=storage_option)\n",
    "\n",
    "            # Evaluate\n",
    "            result_pack = evaluate_one_target(H, testdata = test_data, methods_list = [\"Logistic Regression\", \"Random Forest\"], target = target_id)\n",
    "\n",
    "            # Load to staging package\n",
    "            data_pack = {\n",
    "                'run_id': run_id,\n",
    "                'target_id': target_id,\n",
    "                'summary': {}\n",
    "            }\n",
    "            for method in result_pack.keys():\n",
    "                data_pack[method] = result_pack[method].to_dict(orient='index')\n",
    "\n",
    "                for metric in result_pack[method].columns:\n",
    "                    if str(metric).isupper():\n",
    "                        # Assume all metrics are upper case-noted columns\n",
    "                        data_pack['summary'][f'{method} Mean {metric}'] = np.mean(result_pack[method][metric].values)\n",
    "                        data_pack['summary'][f'{method} Median {metric}'] = np.median(result_pack[method][metric].values)\n",
    "                        data_pack['summary'][f'{method} Std {metric}'] = np.std(result_pack[method][metric].values)\n",
    "                        data_pack['summary'][f'{method} Max {metric}'] = np.max(result_pack[method][metric].values)\n",
    "                        data_pack['summary'][f'{method} Min {metric}'] = np.min(result_pack[method][metric].values)\n",
    "\n",
    "            # Log to MLFlow\n",
    "            for key in data_pack['summary'].keys():\n",
    "                if 'Mean AUROC' in key: mlflow.log_metric(f'{target_id} {key}', data_pack['summary'][key])\n",
    "                if 'Mean MCC' in key: mlflow.log_metric(f'{target_id} {key}', data_pack['summary'][key])\n",
    "        \n",
    "        \n",
    "            # Save to MongoDB\n",
    "            collection.insert_one(data_pack)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mofa.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
